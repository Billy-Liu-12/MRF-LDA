{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install numpy num2words nltk pandas Observations gensim\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# !pip install -U textblob\n",
    "# !python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from num2words import num2words\n",
    "from collections import Counter\n",
    "from scipy import spatial\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from scipy.special import gammaln\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from sklearn.manifold import TSNE\n",
    "from pylab import savefig\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.stats import chi2\n",
    "\n",
    "import gzip\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import imp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import pickle\n",
    "# import lda2\n",
    "import lda_informativeness_usecase as lda\n",
    "import scipy\n",
    "import operator\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import copy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lda_informativeness_usecase' from 'lda_informativeness_usecase.pyc'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.reload(lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numbers(k):\n",
    "    for i in range(len(k)):\n",
    "        try:\n",
    "            num2words(int(k[i]))\n",
    "            k[i] = \" \"\n",
    "        except:\n",
    "            pass\n",
    "    return k\n",
    "\n",
    "def get_cosine(a, b):\n",
    "    return 1 - spatial.distance.cosine(a, b)\n",
    "\n",
    "def preprocess(pd):\n",
    "    pd = pd.str.lower()\n",
    "    pd = pd.str.replace('[{}]'.format('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\n\\t'), ' ')\n",
    "    pd = pd.apply(lambda x: [w for w in w_tokenizer.tokenize(x)])\n",
    "    pd = pd.apply(lambda x: convert_numbers(x))\n",
    "    pd = pd.str.join(' ')\n",
    "    \n",
    "    pd = pd.apply(lambda x: [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(x)])    \n",
    "    pd = pd.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    return pd\n",
    "\n",
    "def processReviews(reviews, window=5, MAX_VOCAB_SIZE=1000):\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\",tokenizer=None)\n",
    "    count_matrix = vectorizer.fit_transform(reviews)\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(reviews)\n",
    "    words = vectorizer.get_feature_names()\n",
    "    vocabulary = dict(zip(words,np.arange(len(words))))\n",
    "    inv_vocabulary = dict(zip(np.arange(len(words)),words))\n",
    "    return count_matrix.toarray(), tfidf_matrix.toarray(), vocabulary, words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coherence_score(X, topic_sentiment_df):\n",
    "    X[X>1] = 1    \n",
    "    totalcnt = len(topic_sentiment_df)\n",
    "    total = 0\n",
    "    for allwords in topic_sentiment_df:\n",
    "        for word1 in allwords:\n",
    "            for word2 in allwords:\n",
    "                if word1 != word2:\n",
    "                    ind1 = vocabulary[word1]\n",
    "                    ind2 = vocabulary[word2]\n",
    "                    total += np.log((np.matmul(X[:,ind1].T, X[:,ind2]) + 1.0)/np.sum(X[:,ind2]))\n",
    "    return total/(2*totalcnt)\n",
    "\n",
    "def kl_score(pk,qk):\n",
    "    return (scipy.stats.entropy(pk,qk)*.5 + scipy.stats.entropy(qk,pk)*.5)\n",
    "\n",
    "def get_hscore(dt_distribution, X, k):\n",
    "    testlen = X.shape[0]\n",
    "    all_kl_scores = np.zeros((testlen, testlen))\n",
    "    for i in range(testlen-1):\n",
    "        for j in range(i+1,testlen):\n",
    "            score = kl_score(dt_distribution[i],dt_distribution[j])\n",
    "            all_kl_scores[i,j] = score\n",
    "            all_kl_scores[j,i] = score\n",
    "\n",
    "    dt = np.zeros((X.shape[0], k))\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        dt[i, dt_distribution[i].argmax()]=1\n",
    "\n",
    "    intradist = 0\n",
    "    for i in range(k):\n",
    "        cnt = dt[:,i].sum()\n",
    "        tmp = np.outer(dt[:,i],dt[:,i])\n",
    "        tmp = tmp * all_kl_scores\n",
    "        intradist += tmp.sum()*1.0/(cnt*(cnt-1))\n",
    "    intradist = intradist/k\n",
    "    \n",
    "\n",
    "    interdist = 0\n",
    "    for i in range(k):\n",
    "       for j in range(k):\n",
    "           if i != j:\n",
    "             cnt_i = dt[:,i].sum()\n",
    "             cnt_j = dt[:,j].sum()\n",
    "             tmp = np.outer(dt[:,i], dt[:,j])\n",
    "             tmp = tmp * all_kl_scores\n",
    "             interdist += tmp.sum()*1.0/(cnt_i*cnt_j)\n",
    "    interdist = interdist/(k*(k-1))\n",
    "    return intradist/interdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simple_TSNE(data, title):\n",
    "    X_embedded = TSNE(n_components=2).fit_transform(data)\n",
    "\n",
    "    X = np.array([i[0] for i in X_embedded])\n",
    "    Y = np.array([i[1] for i in X_embedded])\n",
    "    plt.scatter(X, Y)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=(1.04,0))\n",
    "    plt.show()\n",
    "\n",
    "def plot_TSNE(dt_distribution, C, labels, printit, title):\n",
    "    X_embedded = TSNE(n_components=2).fit_transform(dt_distribution)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    X = np.array([i[0] for i in X_embedded])\n",
    "    Y = np.array([i[1] for i in X_embedded])\n",
    "    for i in range(len(labels)):\n",
    "        xx = X[[np.where(C == i)[0].tolist()]]\n",
    "        yy = Y[[np.where(C == i)[0].tolist()]]\n",
    "        plt.scatter(xx, yy, label=labels[i])\n",
    "\n",
    "    if printit:\n",
    "        for idx, p in enumerate(X_embedded):\n",
    "            plt.annotate(idx, (p[0], p[1]))\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=(1.04,0))\n",
    "    plt.show()\n",
    "    \n",
    "def get_doc_details(num):\n",
    "    print(\"label: \", C[num])\n",
    "    print(dataset[9].values[num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "('Done.', 1917494, ' words loaded!')\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = loadGloveModel(\"glove.42B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# embeddings_index = gensim.models.KeyedVectors.load_word2vec_format('glove.42B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse(path):\n",
    "#     g = gzip.open(path, 'r')\n",
    "#     for l in g:\n",
    "#         yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "  g = gzip.open(path, 'r')\n",
    "  for l in g:\n",
    "    yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = parse(\"reviews_Musical_Instruments_5.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(list(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reviews = dataset['reviewText'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ratings = dataset['overall'].values[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[8] = preprocess(dataset['reviewText'])\n",
    "dataset[9] = dataset[8].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix, tfidf_matrix, vocabulary, words = processReviews(dataset[9].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 1747), (200, 1747))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_matrix.shape, tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_threshold = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs_edges, ignored, taken, count = [], [], [], 0\n",
    "for idx, doc in enumerate(dataset[8].values):\n",
    "    edges = []\n",
    "    for i in doc:\n",
    "        for j in doc:\n",
    "            if i != j:\n",
    "                try:\n",
    "                    a = embeddings_index[i]\n",
    "                    b = embeddings_index[j]\n",
    "                    if get_cosine(a, b) > edges_threshold and (vocabulary[i], vocabulary[j]) not in edges and (vocabulary[j], vocabulary[i]) not in edges:\n",
    "                        edges.append((vocabulary[i], vocabulary[j]))\n",
    "                except:\n",
    "                    try:\n",
    "                        embeddings_index[i]\n",
    "                        taken.append(i)\n",
    "                    except:\n",
    "                        ignored.append(i)\n",
    "                    try:\n",
    "                        embeddings_index[j]\n",
    "                    except:\n",
    "                        ignored.append(j)\n",
    "                        taken.append(j)\n",
    "                    pass\n",
    "    docs_edges.append(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_dict = {}\n",
    "for i in docs_edges:\n",
    "    for j in i:\n",
    "        try:\n",
    "            edge_dict[j[0]] += [j[1]]\n",
    "            edge_dict[j[1]] += [j[0]]\n",
    "        except:\n",
    "            edge_dict[j[0]] = [j[1]]\n",
    "            edge_dict[j[1]] = [j[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in edge_dict.keys():\n",
    "    edge_dict[i] = list(set(edge_dict[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(edge_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = N_TOPICS = 9\n",
    "N_SENTIMENT = 5\n",
    "lambda_param = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lda_informativeness_usecase' from 'lda_informativeness_usecase.pyc'>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.reload(lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = lda.LdaSampler(n_sentiment = N_SENTIMENT, n_topics=N_TOPICS, lambda_param=lambda_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "-54939.83616585099\n",
      "1\n",
      "-54460.82211702777\n",
      "2\n",
      "-54835.94562306618\n",
      "3\n",
      "-55149.697556169216\n",
      "4\n",
      "-54613.65120890179\n",
      "5\n",
      "-54404.6180205531\n",
      "6\n",
      "-54678.68869002354\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i)\n",
    "    sampler.run(count_matrix, ratings, edge_dict, maxiter=20)\n",
    "    print(sampler.loglikelihood(docs_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_words = sampler.getTopKWords(5, words)\n",
    "top_words = [t_words[i] for i in t_words.keys()]\n",
    "document_topic = sampler.theta().argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coherence_score(count_matrix, top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_hscore(sampler.theta(), count_matrix, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_generated = sampler.theta().argmax(axis=1)\n",
    "document_word_sampler = np.dot(sampler.theta(), sampler.phi())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing D-T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels = list(xrange(N_TOPICS))\n",
    "# labels = ['Treatment', 'Heart Surgery', 'Symptoms', 'Procedure', 'Pregnency', 'Clinical', 'Tests', 'Cancer', 'Abdomen']\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_TSNE(document_word_sampler, topics_generated, labels, True, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
