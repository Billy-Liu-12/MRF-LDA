{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from num2words import num2words\n",
    "from collections import Counter\n",
    "from scipy import spatial\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from scipy.special import gammaln\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "\n",
    "import operator\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "def convert_numbers(k):\n",
    "    for i in range(len(k)):\n",
    "        try:\n",
    "            num2words(int(k[i]))\n",
    "            k[i] = \" \"\n",
    "        except:\n",
    "            pass\n",
    "    return k\n",
    "def preprocess(pd):\n",
    "    pd = pd.str.lower()\n",
    "    pd = pd.str.replace('[{}]'.format('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\n\\t'), ' ')\n",
    "    pd = pd.apply(lambda x: [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(x)])\n",
    "    pd = pd.apply(lambda x: convert_numbers(x))\n",
    "    pd = pd.str.join(' ')\n",
    "    pd = pd.str.replace('[{}]'.format(string.punctuation), ' ')\n",
    "    \n",
    "    pd = pd.apply(lambda x: [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(x)])    \n",
    "    pd = pd.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    return pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_excel(\"OCR Output.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[6] = preprocess(dataset[\"Text\"])\n",
    "dataset[7] = dataset[6].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[7].values\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data)\n",
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "n_top_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LatentDirichletAllocation(n_topics=k, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/python2/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "id_topic = model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = {}\n",
    "for topic, comp in enumerate(model.components_):\n",
    "    word_idx = np.argsort(comp)[::-1][:n_top_words]\n",
    "    topic_words[topic] = [vocab[i] for i in word_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Topic: 0', u'discharge, case, hospital, day, tab, wa, patient, admitted, name, date')\n",
      "('Topic: 1', u'discharge, wa, case, tab, lad, mg, dr, consultant, day, bd')\n",
      "('Topic: 2', u'discharge, hospital, dr, admission, case, date, day, vitals, treatment, patient')\n",
      "('Topic: 3', u'discharge, hospital, name, case, day, dr, tab, date, time, summary')\n",
      "('Topic: 4', u'discharge, case, tab, summary, mg, dr, wa, daily, severe, consultant')\n"
     ]
    }
   ],
   "source": [
    "for topic, words in topic_words.items():\n",
    "    print('Topic: %d' % topic, ', '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
