{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install numpy num2words nltk pandas Observations gensim\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from num2words import num2words\n",
    "from collections import Counter\n",
    "from scipy import spatial\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from scipy.special import gammaln\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "\n",
    "import operator\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "embeddings_index = gensim.models.KeyedVectors.load_word2vec_format('pubmed2018_w2v_200D/pubmed2018_w2v_200D.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_index = {}\n",
    "# f = open('glove.6B.300d.txt')\n",
    "# for line in f:\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "#     embeddings_index[word] = coefs\n",
    "# f.close()\n",
    "\n",
    "# print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding = \"ISO-8859-1\", header=None)\n",
    "# dataset = dataset.head(1000)\n",
    "# dataset[6] = preprocess(dataset[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_excel(\"OCR Output.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numbers(k):\n",
    "    for i in range(len(k)):\n",
    "        try:\n",
    "            num2words(int(k[i]))\n",
    "            k[i] = \" \"\n",
    "        except:\n",
    "            pass\n",
    "    return k\n",
    "\n",
    "def get_cosine(a, b):\n",
    "    return 1 - spatial.distance.cosine(a, b)\n",
    "\n",
    "def preprocess(pd):\n",
    "    pd = pd.apply(lambda x: str(TextBlob(x).correct()).decode('utf-8').strip())\n",
    "    pd = pd.str.lower()\n",
    "    pd = pd.str.replace('[{}]'.format('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\n\\t'), ' ')\n",
    "    pd = pd.apply(lambda x: [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(x)])\n",
    "    pd = pd.apply(lambda x: convert_numbers(x))\n",
    "    pd = pd.str.join(' ')\n",
    "    pd = pd.str.replace('[{}]'.format(string.punctuation), ' ')\n",
    "    \n",
    "    pd = pd.apply(lambda x: [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(x)])    \n",
    "    pd = pd.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "#     pd = pd.apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "    return pd\n",
    "\n",
    "def processReviews(reviews, window=5, MAX_VOCAB_SIZE=1000):\n",
    "#     vectorizer = SkipGramVectorizer(analyzer=\"word\",stop_words=\"english\",\n",
    "#                                          max_features=MAX_VOCAB_SIZE,max_df=.75,min_df=10, k = window,ngram_range=(1,2))\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\",tokenizer=None)\n",
    "#                                       , preprocessor=None,stop_words=\"english\",max_features=MAX_VOCAB_SIZE,max_df=.5,min_df=5)\n",
    "    train_data_features = vectorizer.fit_transform(reviews)\n",
    "    words = vectorizer.get_feature_names()\n",
    "    vocabulary = dict(zip(words,np.arange(len(words))))\n",
    "    inv_vocabulary = dict(zip(np.arange(len(words)),words))\n",
    "    wordOccurenceMatrix = train_data_features.toarray()\n",
    "    return wordOccurenceMatrix, vocabulary, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[6] = preprocess(dataset['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {}\n",
    "for i in dataset[6]:\n",
    "    for word in Counter(i).keys():\n",
    "        try:\n",
    "            df[word] += 1\n",
    "        except:\n",
    "            df[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_x = sorted(df.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_df = [i[0] for i in sorted_x[:200]]\n",
    "dataset[7] = dataset[6].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_clean(data):\n",
    "    text = \"\"\n",
    "    for i in data.split(\" \"):\n",
    "        if i not in high_df and len(i)>2:\n",
    "            text += i +\" \"\n",
    "        else:\n",
    "            pass\n",
    "    return text.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[7] = dataset[7].apply(lambda x: deep_clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix, vocabulary, words = processReviews(dataset[7].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "docs_edges = []\n",
    "count = 0\n",
    "ignored = []\n",
    "taken = []\n",
    "for idx, doc in enumerate(dataset[6].values):\n",
    "    edges = []\n",
    "    for i in doc:\n",
    "        for j in doc:\n",
    "            if i != j:\n",
    "                try:\n",
    "                    a = embeddings_index[i]\n",
    "                    b = embeddings_index[j]\n",
    "                    if get_cosine(a, b) > 0.5:\n",
    "                        edges.append((vocabulary[i], vocabulary[j]))\n",
    "                except:\n",
    "                    try:\n",
    "                        embeddings_index[i]\n",
    "                        taken.append(i)\n",
    "                    except:\n",
    "                        ignored.append(i)\n",
    "                    try:\n",
    "                        embeddings_index[j]\n",
    "                    except:\n",
    "                        ignored.append(j)\n",
    "                        taken.append(j)\n",
    "                    pass\n",
    "    docs_edges.append(edges)\n",
    "    \n",
    "    if idx % 10 == 0:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U textblob\n",
    "# !python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_dict = {}\n",
    "for i in docs_edges:\n",
    "    for j in i:\n",
    "#         print(j)\n",
    "        try:\n",
    "            edge_dict[j[0]] += [j[1]]\n",
    "        except:\n",
    "            edge_dict[j[0]] = [j[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lda2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_param = 0.8\n",
    "N_TOPICS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(lda2)\n",
    "sampler = lda2.LdaSampler(n_topics=N_TOPICS, lambda_param=lambda_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Likelihood -71731.7146030965\n",
      "[u'today', u'realli', u'like', u'miss', u'wa']\n",
      "[u'home', u'one', u'get', u'miss', u'sad']\n",
      "[u'go', u'much', u'show', u'see', u'tomorrow']\n",
      "[u'quot', u'sick', u'one', u'work', u'wa']\n",
      "[u'gt', u'one', u'http', u'work', u'want']\n",
      "[u'quot', u'well', u'realli', u'today', u'tonight']\n",
      "[u'earli', u'know', u'get', u'work', u'bed']\n",
      "[u'alreadi', u'peopl', u'good', u'day', u'sad']\n",
      "[u'like', u'need', u'miss', u'get', u'time']\n",
      "[u'good', u'realli', u'bad', u'work', u'sleep']\n",
      "Iteration 1\n",
      "Likelihood -68751.07829038508\n",
      "[u'miss', u'wish', u'watch', u'im', u'realli']\n",
      "[u'good', u'go', u'sad', u'miss', u'day']\n",
      "[u'tri', u'miss', u'sorri', u'go', u'like']\n",
      "[u'world', u'much', u'sad', u'work', u'get']\n",
      "[u'work', u'get', u'go', u'know', u'one']\n",
      "[u'like', u'today', u'tonight', u'well', u'work']\n",
      "[u'still', u'bed', u'cri', u'quot', u'work']\n",
      "[u'hate', u'home', u'wa', u'got', u'good']\n",
      "[u'better', u'like', u'time', u'miss', u'get']\n",
      "[u'night', u'find', u'could', u'hurt', u'sleep']\n",
      "Iteration 2\n",
      "Likelihood -67553.5590629881\n",
      "[u'miss', u'break', u'got', u'bad', u'com']\n",
      "[u'go', u'sad', u'miss', u'day', u'watch']\n",
      "[u'sick', u'feel', u'sorri', u'see', u'miss']\n",
      "[u'realli', u'last', u'work', u'sad', u'one']\n",
      "[u'one', u'ha', u'go', u'get', u'know']\n",
      "[u'today', u'like', u'back', u'time', u'go']\n",
      "[u'bed', u'quot', u'im', u'work', u'sleep']\n",
      "[u'home', u'hate', u'got', u'new', u'good']\n",
      "[u'like', u'much', u'amp', u'miss', u'get']\n",
      "[u'night', u'hurt', u'could', u'day', u'go']\n",
      "Iteration 3\n",
      "Likelihood -66974.68029051297\n",
      "[u'wa', u'com', u'bad', u'miss', u'got']\n",
      "[u'miss', u'hate', u'watch', u'wa', u'day']\n",
      "[u'sorri', u'miss', u'feel', u'see', u'sick']\n",
      "[u'realli', u'work', u'sad', u'call', u'get']\n",
      "[u'back', u'go', u'know', u'wa', u'get']\n",
      "[u'back', u'sorri', u'day', u'time', u'work']\n",
      "[u'sleep', u'bed', u'im', u'work', u'quot']\n",
      "[u'wish', u'lot', u'got', u'good', u'wa']\n",
      "[u'amp', u'much', u'thing', u'com', u'get']\n",
      "[u'could', u'night', u'get', u'hurt', u'sleep']\n",
      "Iteration 4\n",
      "Likelihood -66432.65389302072\n",
      "[u'cri', u'miss', u'watch', u'com', u'want']\n",
      "[u'go', u'morn', u'watch', u'sad', u'wa']\n",
      "[u'hate', u'go', u'see', u'sick', u'feel']\n",
      "[u'got', u'wa', u'get', u'realli', u'amp']\n",
      "[u'one', u'work', u'think', u'know', u'want']\n",
      "[u'lost', u'hour', u'feel', u'go', u'work']\n",
      "[u'bed', u'found', u'get', u'quot', u'im']\n",
      "[u'lot', u'day', u'good', u'week', u'wa']\n",
      "[u'thing', u'com', u'quot', u'ha', u'http']\n",
      "[u'time', u'hurt', u'find', u'still', u'go']\n",
      "Iteration 5\n",
      "Likelihood -65975.5357604731\n",
      "[u'realli', u'cri', u'wa', u'want', u'com']\n",
      "[u'home', u'morn', u'sad', u'watch', u'wa']\n",
      "[u'tomorrow', u'go', u'sick', u'hate', u'feel']\n",
      "[u'still', u'amp', u'wa', u'well', u'get']\n",
      "[u'dont', u'know', u'need', u'get', u'think']\n",
      "[u'lost', u'hour', u'go', u'time', u'back']\n",
      "[u'tomorrow', u'ha', u'sleep', u'quot', u'miss']\n",
      "[u'day', u'wa', u'ha', u'good', u'know']\n",
      "[u'miss', u'com', u'wa', u'quot', u'http']\n",
      "[u'hurt', u'time', u'realli', u'find', u'sleep']\n",
      "Iteration 6\n",
      "Likelihood -65790.85187473262\n",
      "[u'love', u'http', u'want', u'got', u'com']\n",
      "[u'morn', u'sad', u'go', u'watch', u'wa']\n",
      "[u'hate', u'miss', u'tomorrow', u'sick', u'like']\n",
      "[u'got', u'home', u'work', u'sad', u'well']\n",
      "[u'dont', u'today', u'know', u'think', u'want']\n",
      "[u'feel', u'day', u'like', u'hour', u'go']\n",
      "[u'miss', u'go', u'found', u'quot', u'ha']\n",
      "[u'ha', u'love', u'week', u'good', u'new']\n",
      "[u'go', u'thing', u'com', u'quot', u'http']\n",
      "[u'hurt', u'find', u'realli', u'go', u'day']\n",
      "Iteration 7\n",
      "Likelihood -65442.35023442583\n",
      "[u'time', u'http', u'got', u'wa', u'com']\n",
      "[u'morn', u'go', u'sad', u'watch', u'wa']\n",
      "[u'see', u'hate', u'feel', u'tomorrow', u'like']\n",
      "[u'get', u'still', u'well', u'work', u'home']\n",
      "[u'one', u'oh', u'know', u'want', u'think']\n",
      "[u'sleep', u'go', u'feel', u'work', u'like']\n",
      "[u'miss', u'get', u'ha', u'tomorrow', u'quot']\n",
      "[u'wait', u'good', u'wa', u'got', u'new']\n",
      "[u'time', u'sleep', u'thing', u'quot', u'http']\n",
      "[u'made', u'find', u'day', u'realli', u'sleep']\n",
      "Iteration 8\n",
      "Likelihood -65150.52232121488\n",
      "[u'time', u'got', u'http', u'wa', u'com']\n",
      "[u'fun', u'morn', u'sad', u'watch', u'wa']\n",
      "[u'tomorrow', u'see', u'feel', u'sick', u'hate']\n",
      "[u'sad', u'well', u'get', u'work', u'home']\n",
      "[u'get', u'know', u'need', u'oh', u'want']\n",
      "[u'hour', u'work', u'day', u'like', u'feel']\n",
      "[u'miss', u'quot', u'go', u'ha', u'tomorrow']\n",
      "[u'wa', u'wait', u'new', u'got', u'ha']\n",
      "[u'time', u'com', u'thing', u'http', u'get']\n",
      "[u'find', u'still', u'sleep', u'realli', u'day']\n",
      "Iteration 9\n",
      "Likelihood -65200.05877381178\n",
      "[u'got', u'http', u'com', u'wa', u'time']\n",
      "[u'morn', u'ha', u'sad', u'watch', u'wa']\n",
      "[u'like', u'feel', u'sick', u'back', u'tomorrow']\n",
      "[u'wa', u'amp', u'work', u'home', u'well']\n",
      "[u'oh', u'know', u'need', u'want', u'get']\n",
      "[u'hour', u'work', u'like', u'sleep', u'night']\n",
      "[u'ha', u'tomorrow', u'found', u'go', u'quot']\n",
      "[u'wa', u'day', u'good', u'got', u'ha']\n",
      "[u'lost', u'time', u'com', u'http', u'quot']\n",
      "[u'realli', u'get', u'made', u'find', u'day']\n",
      "Iteration 10\n",
      "Likelihood -64957.19614217124\n",
      "[u'http', u'love', u'want', u'com', u'wa']\n",
      "[u'morn', u'ha', u'sad', u'watch', u'wa']\n",
      "[u'see', u'go', u'like', u'back', u'feel']\n",
      "[u'week', u'wa', u'home', u'amp', u'work']\n",
      "[u'know', u'oh', u'need', u'think', u'get']\n",
      "[u'tonight', u'hour', u'work', u'night', u'sleep']\n",
      "[u'poor', u'found', u'tomorrow', u'quot', u'go']\n",
      "[u'new', u'got', u'wa', u'hous', u'ha']\n",
      "[u'lost', u'miss', u'com', u'http', u'quot']\n",
      "[u'realli', u'get', u'find', u'made', u'day']\n",
      "Iteration 11\n",
      "Likelihood -64941.71008646328\n",
      "[u'love', u'time', u'want', u'http', u'com']\n",
      "[u'ha', u'morn', u'sad', u'watch', u'wa']\n",
      "[u'tomorrow', u'sick', u'like', u'feel', u'go']\n",
      "[u'sad', u'done', u'home', u'work', u'amp']\n",
      "[u'need', u'know', u'oh', u'want', u'think']\n",
      "[u'time', u'come', u'hour', u'night', u'sleep']\n",
      "[u'new', u'miss', u'go', u'found', u'quot']\n",
      "[u'wish', u'day', u'good', u'got', u'wa']\n",
      "[u'com', u'much', u'lost', u'http', u'quot']\n",
      "[u'wa', u'still', u'find', u'realli', u'day']\n",
      "Iteration 12\n",
      "Likelihood -64949.65412287008\n",
      "[u'hear', u'love', u'today', u'want', u'wa']\n",
      "[u'im', u'morn', u'sad', u'watch', u'wa']\n",
      "[u'back', u'like', u'feel', u'tomorrow', u'hate']\n",
      "[u'season', u'work', u'done', u'home', u'sad']\n",
      "[u'like', u'need', u'oh', u'get', u'want']\n",
      "[u'hour', u'come', u'go', u'time', u'night']\n",
      "[u'miss', u'know', u'found', u'ha', u'quot']\n",
      "[u'new', u'play', u'wish', u'good', u'ha']\n",
      "[u'twitter', u'miss', u'com', u'http', u'get']\n",
      "[u'realli', u'find', u'day', u'get', u'wa']\n",
      "Iteration 13\n",
      "Likelihood -64516.9000876042\n",
      "[u'want', u'hear', u'com', u'http', u'love']\n",
      "[u'go', u'sad', u'im', u'watch', u'miss']\n",
      "[u'back', u'like', u'see', u'tomorrow', u'hate']\n",
      "[u'world', u'done', u'day', u'one', u'work']\n",
      "[u'one', u'need', u'oh', u'get', u'think']\n",
      "[u'feel', u'hour', u'work', u'time', u'sleep']\n",
      "[u'ha', u'found', u'go', u'miss', u'quot']\n",
      "[u'week', u'know', u'new', u'good', u'ha']\n",
      "[u'much', u'miss', u'com', u'http', u'quot']\n",
      "[u'go', u'good', u'realli', u'wa', u'get']\n",
      "Iteration 14\n",
      "Likelihood -64367.25681703925\n",
      "[u'hear', u'cri', u'feel', u'wa', u'com']\n",
      "[u'fun', u'wanna', u'sad', u'watch', u'miss']\n",
      "[u'sick', u'hope', u'like', u'hate', u'tomorrow']\n",
      "[u'sad', u'last', u'get', u'work', u'one']\n",
      "[u'need', u'know', u'one', u'think', u'get']\n",
      "[u'day', u'come', u'time', u'sleep', u'hour']\n",
      "[u'found', u'poor', u'miss', u'ha', u'quot']\n",
      "[u'play', u'wa', u'got', u'ha', u'new']\n",
      "[u'much', u'com', u'miss', u'quot', u'http']\n",
      "[u'good', u'go', u'wa', u'find', u'realli']\n",
      "Iteration 15\n",
      "Likelihood -64546.3274349111\n",
      "[u'thought', u'bad', u'want', u'hear', u'com']\n",
      "[u'sad', u'im', u'go', u'watch', u'miss']\n",
      "[u'back', u'feel', u'like', u'hate', u'tomorrow']\n",
      "[u'last', u'work', u'time', u'wa', u'home']\n",
      "[u'oh', u'need', u'know', u'think', u'get']\n",
      "[u'come', u'work', u'hour', u'sleep', u'day']\n",
      "[u'poor', u'go', u'miss', u'ha', u'quot']\n",
      "[u'home', u'new', u'got', u'ha', u'wa']\n",
      "[u'miss', u'much', u'com', u'get', u'quot']\n",
      "[u'wa', u'find', u'still', u'sleep', u'get']\n",
      "Iteration 16\n",
      "Likelihood -64616.673378765234\n",
      "[u'http', u'would', u'time', u'hear', u'com']\n",
      "[u'look', u'lt', u'wish', u'watch', u'miss']\n",
      "[u'hope', u'see', u'hate', u'like', u'tomorrow']\n",
      "[u'get', u'wa', u'sad', u'well', u'one']\n",
      "[u'need', u'know', u'oh', u'get', u'think']\n",
      "[u'feel', u'hour', u'sleep', u'night', u'work']\n",
      "[u'found', u'poor', u'ha', u'miss', u'quot']\n",
      "[u'news', u'wa', u'know', u'watch', u'ha']\n",
      "[u'thing', u'get', u'com', u'miss', u'http']\n",
      "[u'find', u'take', u'get', u'still', u'realli']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17\n",
      "Likelihood -64432.636504315364\n",
      "[u'time', u'sorri', u'hear', u'http', u'com']\n",
      "[u'look', u'wish', u'lt', u'watch', u'miss']\n",
      "[u'like', u'feel', u'see', u'tomorrow', u'hate']\n",
      "[u'work', u'sorri', u'sad', u'one', u'wa']\n",
      "[u'need', u'oh', u'know', u'think', u'get']\n",
      "[u'feel', u'come', u'hour', u'work', u'night']\n",
      "[u'read', u'found', u'poor', u'quot', u'miss']\n",
      "[u'meet', u'news', u'whole', u'watch', u'good']\n",
      "[u'twitter', u'get', u'com', u'miss', u'http']\n",
      "[u'get', u'still', u'find', u'today', u'realli']\n",
      "Iteration 18\n",
      "Likelihood -64492.12533592274\n",
      "[u'hear', u'com', u'cri', u'http', u'wa']\n",
      "[u'start', u'morn', u'lt', u'watch', u'miss']\n",
      "[u'tomorrow', u'feel', u'hope', u'see', u'hate']\n",
      "[u'wa', u'sorri', u'sad', u'get', u'time']\n",
      "[u'need', u'know', u'get', u'oh', u'think']\n",
      "[u'night', u'hour', u'feel', u'work', u'sleep']\n",
      "[u'poor', u'new', u'go', u'miss', u'quot']\n",
      "[u'meet', u'new', u'make', u'watch', u'good']\n",
      "[u'twitter', u'get', u'miss', u'com', u'http']\n",
      "[u'still', u'today', u'get', u'find', u'realli']\n",
      "Iteration 19\n",
      "Likelihood -64441.034453045824\n",
      "[u'hear', u'http', u'sorri', u'com', u'wa']\n",
      "[u'sad', u'thing', u'lt', u'watch', u'miss']\n",
      "[u'hope', u'feel', u'see', u'tomorrow', u'hate']\n",
      "[u'sorri', u'well', u'get', u'wa', u'one']\n",
      "[u'like', u'know', u'oh', u'think', u'get']\n",
      "[u'sleep', u'come', u'go', u'work', u'night']\n",
      "[u'girl', u'poor', u'read', u'quot', u'new']\n",
      "[u'play', u'make', u'know', u'watch', u'good']\n",
      "[u'get', u'twitter', u'com', u'http', u'miss']\n",
      "[u'lost', u'realli', u'today', u'find', u'wa']\n",
      "Iteration 20\n",
      "Likelihood -64395.69393843425\n",
      "[u'bad', u'com', u'hear', u'http', u'love']\n",
      "[u'thing', u'lt', u'watch', u'morn', u'miss']\n",
      "[u'need', u'see', u'tomorrow', u'feel', u'hate']\n",
      "[u'week', u'wa', u'sorri', u'sad', u'one']\n",
      "[u'need', u'know', u'oh', u'think', u'get']\n",
      "[u'got', u'night', u'go', u'day', u'sleep']\n",
      "[u'new', u'read', u'girl', u'poor', u'quot']\n",
      "[u'make', u'new', u'play', u'good', u'watch']\n",
      "[u'twitter', u'much', u'com', u'http', u'miss']\n",
      "[u'find', u'lost', u'today', u'wa', u'realli']\n",
      "Iteration 21\n",
      "Likelihood -64295.716161135075\n",
      "[u'realli', u'hear', u'wa', u'love', u'http']\n",
      "[u'im', u'lt', u'watch', u'morn', u'miss']\n",
      "[u'sick', u'feel', u'tomorrow', u'see', u'go']\n",
      "[u'wait', u'home', u'sad', u'sorri', u'time']\n",
      "[u'like', u'know', u'oh', u'think', u'get']\n",
      "[u'day', u'time', u'night', u'go', u'sleep']\n",
      "[u'read', u'happen', u'poor', u'found', u'quot']\n",
      "[u'new', u'day', u'play', u'good', u'sad']\n",
      "[u'much', u'com', u'twitter', u'http', u'miss']\n",
      "[u'lost', u'school', u'wa', u'today', u'go']\n",
      "Iteration 22\n",
      "Likelihood -64352.17338351491\n",
      "[u'thought', u'wa', u'realli', u'http', u'love']\n",
      "[u'home', u'lt', u'start', u'watch', u'miss']\n",
      "[u'sick', u'feel', u'see', u'go', u'tomorrow']\n",
      "[u'last', u'sad', u'sorri', u'time', u'get']\n",
      "[u'know', u'need', u'get', u'oh', u'think']\n",
      "[u'night', u'day', u'sleep', u'go', u'time']\n",
      "[u'internet', u'made', u'read', u'poor', u'quot']\n",
      "[u'whole', u'day', u'good', u'play', u'ha']\n",
      "[u'twitter', u'com', u'much', u'http', u'miss']\n",
      "[u'go', u'today', u'wa', u'find', u'realli']\n",
      "Iteration 23\n",
      "Likelihood -64202.09518607202\n",
      "[u'work', u'bad', u'today', u'http', u'love']\n",
      "[u'lt', u'look', u'morn', u'watch', u'miss']\n",
      "[u'see', u'hate', u'tomorrow', u'like', u'feel']\n",
      "[u'last', u'sorri', u'time', u'get', u'sad']\n",
      "[u'know', u'need', u'oh', u'get', u'think']\n",
      "[u'hour', u'day', u'night', u'sleep', u'time']\n",
      "[u'ha', u'read', u'quot', u'internet', u'found']\n",
      "[u'day', u'play', u'week', u'new', u'ha']\n",
      "[u'get', u'twitter', u'com', u'http', u'miss']\n",
      "[u'lost', u'school', u'find', u'today', u'go']\n",
      "Iteration 24\n",
      "Likelihood -64414.97190923802\n",
      "[u'wa', u'bad', u'realli', u'com', u'http']\n",
      "[u'start', u'lt', u'morn', u'watch', u'miss']\n",
      "[u'feel', u'sick', u'tomorrow', u'like', u'go']\n",
      "[u'man', u'sad', u'get', u'sorri', u'time']\n",
      "[u'much', u'know', u'oh', u'get', u'think']\n",
      "[u'sleep', u'feel', u'time', u'night', u'day']\n",
      "[u'happen', u'poor', u'internet', u'read', u'found']\n",
      "[u'make', u'day', u'play', u'ha', u'new']\n",
      "[u'get', u'twitter', u'com', u'http', u'miss']\n",
      "[u'find', u'get', u'lost', u'school', u'today']\n",
      "Iteration 25\n",
      "Likelihood -64179.35922849651\n",
      "[u'today', u'hear', u'realli', u'com', u'http']\n",
      "[u'start', u'morn', u'lt', u'watch', u'miss']\n",
      "[u'tomorrow', u'sick', u'go', u'like', u'feel']\n",
      "[u'man', u'home', u'get', u'sad', u'time']\n",
      "[u'know', u'need', u'oh', u'think', u'get']\n",
      "[u'hour', u'sleep', u'night', u'time', u'day']\n",
      "[u'know', u'read', u'poor', u'found', u'sad']\n",
      "[u'week', u'know', u'ha', u'play', u'new']\n",
      "[u'twitter', u'get', u'com', u'http', u'miss']\n",
      "[u'lost', u'find', u'school', u'today', u'wa']\n",
      "Iteration 26\n",
      "Likelihood -64180.227228072006\n",
      "[u'hear', u'thought', u'realli', u'http', u'com']\n",
      "[u'look', u'lt', u'morn', u'watch', u'miss']\n",
      "[u'see', u'sick', u'tomorrow', u'like', u'feel']\n",
      "[u'last', u'get', u'sad', u'home', u'time']\n",
      "[u'im', u'need', u'oh', u'get', u'think']\n",
      "[u'hour', u'time', u'night', u'day', u'sleep']\n",
      "[u'happen', u'poor', u'found', u'wa', u'quot']\n",
      "[u'new', u'sad', u'play', u'know', u'ha']\n",
      "[u'com', u'twitter', u'get', u'miss', u'http']\n",
      "[u'get', u'school', u'find', u'lost', u'today']\n",
      "Iteration 27\n",
      "Likelihood -63965.52649543605\n",
      "[u'hear', u'today', u'realli', u'com', u'http']\n",
      "[u'morn', u'look', u'lt', u'watch', u'miss']\n",
      "[u'go', u'see', u'like', u'tomorrow', u'feel']\n",
      "[u'sad', u'last', u'day', u'home', u'time']\n",
      "[u'oh', u'know', u'need', u'get', u'want']\n",
      "[u'hour', u'go', u'sleep', u'night', u'time']\n",
      "[u'ha', u'poor', u'found', u'sad', u'quot']\n",
      "[u'new', u'ha', u'play', u'know', u'sad']\n",
      "[u'twitter', u'com', u'get', u'http', u'miss']\n",
      "[u'get', u'school', u'lost', u'today', u'find']\n",
      "Iteration 28\n",
      "Likelihood -64042.29483663388\n",
      "[u'first', u'hear', u'com', u'http', u'realli']\n",
      "[u'wish', u'morn', u'work', u'watch', u'miss']\n",
      "[u'go', u'like', u'see', u'tomorrow', u'feel']\n",
      "[u'week', u'sorri', u'home', u'day', u'one']\n",
      "[u'know', u'much', u'oh', u'get', u'think']\n",
      "[u'come', u'time', u'go', u'sleep', u'night']\n",
      "[u'poor', u'new', u'found', u'sad', u'wa']\n",
      "[u'new', u'play', u'know', u'good', u'sad']\n",
      "[u'twitter', u'get', u'com', u'http', u'miss']\n",
      "[u'work', u'lost', u'get', u'find', u'go']\n",
      "Iteration 29\n",
      "Likelihood -64157.00301829362\n",
      "[u'today', u'hear', u'realli', u'http', u'com']\n",
      "[u'work', u'morn', u'season', u'watch', u'wa']\n",
      "[u'go', u'see', u'like', u'tomorrow', u'feel']\n",
      "[u'got', u'still', u'man', u'home', u'time']\n",
      "[u'much', u'oh', u'need', u'think', u'want']\n",
      "[u'go', u'sleep', u'day', u'time', u'night']\n",
      "[u'found', u'poor', u'new', u'sad', u'wa']\n",
      "[u'week', u'play', u'know', u'sad', u'ha']\n",
      "[u'much', u'twitter', u'com', u'miss', u'http']\n",
      "[u'school', u'lost', u'get', u'find', u'today']\n"
     ]
    }
   ],
   "source": [
    "for it, phi in enumerate(sampler.run(matrix, edge_dict)):\n",
    "    print \"Iteration\", it\n",
    "    print \"Likelihood\", sampler.loglikelihood(docs_edges)\n",
    "    for i in (sampler.getTopKWords(5, words)):\n",
    "        print(sampler.getTopKWords(5, words)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
