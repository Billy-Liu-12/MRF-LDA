{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install numpy num2words nltk pandas Observations\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from num2words import num2words\n",
    "from collections import Counter\n",
    "from scipy import spatial\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from scipy.special import gammaln\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numbers(k):\n",
    "    for i in range(len(k)):\n",
    "        try:\n",
    "            num2words(int(k[i]))\n",
    "            k[i] = \" \"\n",
    "        except:\n",
    "            pass\n",
    "    return k\n",
    "\n",
    "def get_cosine(a, b):\n",
    "    return 1 - spatial.distance.cosine(a, b)\n",
    "\n",
    "def preprocess(pd):\n",
    "    pd = pd.str.lower()\n",
    "    pd = pd.str.replace('[{}]'.format('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\n\\t'), ' ')\n",
    "    pd = pd.apply(lambda x: [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(x)])\n",
    "    pd = pd.apply(lambda x: convert_numbers(x))\n",
    "    pd = pd.str.join(' ')\n",
    "    pd = pd.str.replace('[{}]'.format(string.punctuation), ' ')\n",
    "    \n",
    "    pd = pd.apply(lambda x: [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(x)])    \n",
    "    pd = pd.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    pd = pd.apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "    return pd\n",
    "\n",
    "def processReviews(reviews, window=5, MAX_VOCAB_SIZE=1000):\n",
    "#     vectorizer = SkipGramVectorizer(analyzer=\"word\",stop_words=\"english\",\n",
    "#                                          max_features=MAX_VOCAB_SIZE,max_df=.75,min_df=10, k = window,ngram_range=(1,2))\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\",tokenizer=None)\n",
    "#                                       , preprocessor=None,stop_words=\"english\",max_features=MAX_VOCAB_SIZE,max_df=.5,min_df=5)\n",
    "    train_data_features = vectorizer.fit_transform(reviews)\n",
    "    words = vectorizer.get_feature_names()\n",
    "    vocabulary = dict(zip(words,np.arange(len(words))))\n",
    "    inv_vocabulary = dict(zip(np.arange(len(words)),words))\n",
    "    wordOccurenceMatrix = train_data_features.toarray()\n",
    "    return wordOccurenceMatrix, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding = \"ISO-8859-1\", header=None)\n",
    "dataset = dataset.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[6] = preprocess(dataset[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[7] = dataset[6].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix, vocabulary = processReviews(dataset[7].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "300\n",
      "600\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "docs_edges = []\n",
    "for idx, doc in enumerate(dataset[6].values):\n",
    "    edges = []\n",
    "    for i in doc:\n",
    "        for j in doc:\n",
    "            if i != j:\n",
    "                try:\n",
    "                    a = embeddings_index[i]\n",
    "                    b = embeddings_index[j]\n",
    "                    if get_cosine(a, b) > 0.5:\n",
    "                        edges.append((vocabulary[i], vocabulary[j]))\n",
    "                except:\n",
    "                    pass\n",
    "    docs_edges.append(edges)\n",
    "    \n",
    "    if idx % 300 == 0:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_dict = {}\n",
    "for i in docs_edges:\n",
    "    for j in i:\n",
    "#         print(j)\n",
    "        try:\n",
    "            edge_dict[j[0]] += [j[1]]\n",
    "        except:\n",
    "            edge_dict[j[0]] = [j[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lda2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_param = 0.8\n",
    "N_TOPICS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "imp.reload(lda2)\n",
    "sampler = lda2.LdaSampler(n_topics=N_TOPICS, lambda_param=lambda_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Likelihood -71583.11218166549\n",
      "Iteration 1\n",
      "Likelihood -68842.44884274623\n",
      "Iteration 2\n",
      "Likelihood -67635.13516758756\n",
      "Iteration 3\n",
      "Likelihood -66902.40764089416\n",
      "Iteration 4\n",
      "Likelihood -66252.16934392933\n",
      "Iteration 5\n",
      "Likelihood -65966.3182530556\n",
      "Iteration 6\n",
      "Likelihood -65736.0316593655\n",
      "Iteration 7\n",
      "Likelihood -65455.64990137747\n",
      "Iteration 8\n",
      "Likelihood -65305.02106596165\n",
      "Iteration 9\n",
      "Likelihood -64996.909193830616\n",
      "Iteration 10\n",
      "Likelihood -64955.81816349574\n",
      "Iteration 11\n",
      "Likelihood -64972.66005038114\n",
      "Iteration 12\n",
      "Likelihood -64848.04985113517\n",
      "Iteration 13\n",
      "Likelihood -64728.383807446204\n",
      "Iteration 14\n",
      "Likelihood -64857.22927147303\n",
      "Iteration 15\n",
      "Likelihood -64763.03982860091\n",
      "Iteration 16\n",
      "Likelihood -64538.23236013443\n",
      "Iteration 17\n",
      "Likelihood -64562.26481234656\n",
      "Iteration 18\n",
      "Likelihood -64376.84546127207\n",
      "Iteration 19\n",
      "Likelihood -64417.42715586826\n",
      "Iteration 20\n",
      "Likelihood -64265.471159214285\n",
      "Iteration 21\n",
      "Likelihood -64224.64299657952\n",
      "Iteration 22\n",
      "Likelihood -64290.77760295536\n",
      "Iteration 23\n",
      "Likelihood -64288.18751359549\n",
      "Iteration 24\n",
      "Likelihood -64254.34132148075\n",
      "Iteration 25\n",
      "Likelihood -64228.19821579668\n",
      "Iteration 26\n",
      "Likelihood -64262.525090907475\n",
      "Iteration 27\n",
      "Likelihood -64092.36577020953\n",
      "Iteration 28\n",
      "Likelihood -64168.507384519224\n",
      "Iteration 29\n",
      "Likelihood -64104.30579546383\n"
     ]
    }
   ],
   "source": [
    "for it, phi in enumerate(sampler.run(matrix, edge_dict)):\n",
    "    print \"Iteration\", it\n",
    "    print \"Likelihood\", sampler.loglikelihood(docs_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
