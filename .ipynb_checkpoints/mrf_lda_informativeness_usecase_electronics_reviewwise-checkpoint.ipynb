{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip install numpy num2words nltk pandas Observations gensim\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# !pip install -U textblob\n",
    "# !python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from num2words import num2words\n",
    "from collections import Counter\n",
    "from scipy import spatial\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from scipy.special import gammaln\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from sklearn.manifold import TSNE\n",
    "from pylab import savefig\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.stats import chi2\n",
    "\n",
    "import gzip\n",
    "# import plotly\n",
    "# import plotly.plotly as py\n",
    "# import plotly.graph_objs as go\n",
    "import imp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import pickle\n",
    "import lda_informativeness_usecase as lda\n",
    "import scipy\n",
    "import operator\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import copy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lda_informativeness_usecase' from 'lda_informativeness_usecase.pyc'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.reload(lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numbers(k):\n",
    "    for i in range(len(k)):\n",
    "        try:\n",
    "            num2words(int(k[i]))\n",
    "            k[i] = \" \"\n",
    "        except:\n",
    "            pass\n",
    "    return k\n",
    "\n",
    "def get_cosine(a, b):\n",
    "    return 1 - spatial.distance.cosine(a, b)\n",
    "\n",
    "def preprocess(pd):\n",
    "    pd = pd.str.lower()\n",
    "    pd = pd.str.replace('[{}]'.format('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\n\\t'), ' ')\n",
    "    pd = pd.apply(lambda x: [w for w in w_tokenizer.tokenize(x)])\n",
    "    pd = pd.apply(lambda x: convert_numbers(x))\n",
    "    pd = pd.str.join(' ')\n",
    "    \n",
    "    pd = pd.apply(lambda x: [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(x)])    \n",
    "    pd = pd.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    return pd\n",
    "\n",
    "def processReviews(reviews, window=5, MAX_VOCAB_SIZE=2000):\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\",tokenizer=None, max_df=0.7, max_features=MAX_VOCAB_SIZE)\n",
    "    count_matrix = vectorizer.fit_transform(reviews)\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=MAX_VOCAB_SIZE, max_df=0.7)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(reviews)\n",
    "    words = vectorizer.get_feature_names()\n",
    "    vocabulary = dict(zip(words,np.arange(len(words))))\n",
    "    inv_vocabulary = dict(zip(np.arange(len(words)),words))\n",
    "    return count_matrix.toarray(), tfidf_matrix.toarray(), vocabulary, words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coherence_score(X, topic_sentiment_df):\n",
    "    X[X>1] = 1    \n",
    "    totalcnt = len(topic_sentiment_df)\n",
    "    total = 0\n",
    "    for allwords in topic_sentiment_df:\n",
    "        for word1 in allwords:\n",
    "            for word2 in allwords:\n",
    "                if word1 != word2:\n",
    "                    ind1 = vocabulary[word1]\n",
    "                    ind2 = vocabulary[word2]\n",
    "                    total += np.log((np.matmul(X[:,ind1].T, X[:,ind2]) + 1.0)/np.sum(X[:,ind2]))\n",
    "    return total/(2*totalcnt)\n",
    "\n",
    "def kl_score(pk,qk):\n",
    "    return (scipy.stats.entropy(pk,qk)*.5 + scipy.stats.entropy(qk,pk)*.5)\n",
    "\n",
    "def get_hscore(dt_distribution, X, k):\n",
    "    testlen = X.shape[0]\n",
    "    all_kl_scores = np.zeros((testlen, testlen))\n",
    "    for i in range(testlen-1):\n",
    "        for j in range(i+1,testlen):\n",
    "            score = kl_score(dt_distribution[i],dt_distribution[j])\n",
    "            all_kl_scores[i,j] = score\n",
    "            all_kl_scores[j,i] = score\n",
    "\n",
    "    dt = np.zeros((X.shape[0], k))\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        dt[i, dt_distribution[i].argmax()]=1\n",
    "\n",
    "    intradist = 0\n",
    "    for i in range(k):\n",
    "        cnt = dt[:,i].sum()\n",
    "        tmp = np.outer(dt[:,i],dt[:,i])\n",
    "        tmp = tmp * all_kl_scores\n",
    "        intradist += tmp.sum()*1.0/(cnt*(cnt-1))\n",
    "    intradist = intradist/k\n",
    "    \n",
    "\n",
    "    interdist = 0\n",
    "    for i in range(k):\n",
    "       for j in range(k):\n",
    "           if i != j:\n",
    "             cnt_i = dt[:,i].sum()\n",
    "             cnt_j = dt[:,j].sum()\n",
    "             tmp = np.outer(dt[:,i], dt[:,j])\n",
    "             tmp = tmp * all_kl_scores\n",
    "             interdist += tmp.sum()*1.0/(cnt_i*cnt_j)\n",
    "    interdist = interdist/(k*(k-1))\n",
    "    return intradist/interdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simple_TSNE(data, title):\n",
    "    X_embedded = TSNE(n_components=2).fit_transform(data)\n",
    "\n",
    "    X = np.array([i[0] for i in X_embedded])\n",
    "    Y = np.array([i[1] for i in X_embedded])\n",
    "    plt.scatter(X, Y)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=(1.04,0))\n",
    "    plt.show()\n",
    "\n",
    "def plot_TSNE(dt_distribution, C, labels, printit, title):\n",
    "    X_embedded = TSNE(n_components=2).fit_transform(dt_distribution)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    X = np.array([i[0] for i in X_embedded])\n",
    "    Y = np.array([i[1] for i in X_embedded])\n",
    "    for i in range(len(labels)):\n",
    "        xx = X[[np.where(C == i)[0].tolist()]]\n",
    "        yy = Y[[np.where(C == i)[0].tolist()]]\n",
    "        plt.scatter(xx, yy, label=labels[i])\n",
    "\n",
    "    if printit:\n",
    "        for idx, p in enumerate(X_embedded):\n",
    "            plt.annotate(idx, (p[0], p[1]))\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=(1.04,0))\n",
    "    plt.show()\n",
    "    \n",
    "def get_doc_details(num):\n",
    "    print(\"label: \", C[num])\n",
    "    print(dataset[9].values[num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# embeddings_index = loadGloveModel(\"nongit_resources/glove.42B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_docs = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataset = parse(\"nongit_resources/reviews_Electronics_5.json.gz\")\n",
    "# dataset = pd.DataFrame(list(dataset))\n",
    "# dataset = dataset.head(N_docs)\n",
    "dataset.to_pickle(\"resources/reviews_Electronics_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_pickle(\"resources/reviews_Electronics_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(\"resources/reviews_Electronics_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[['asin', 'helpful', 'overall', 'reviewText']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reviews = dataset['reviewText'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ratings = dataset['overall'].values[:N_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[8] = preprocess(dataset['reviewText'])\n",
    "dataset[9] = dataset[8].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix, tfidf_matrix, vocabulary, words = processReviews(dataset[9].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix.shape, tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset[['asin', 'helpful', 'overall', 'reviewText']]\n",
    "\n",
    "# dataset['n_words'] = dataset['reviewText'].apply(lambda x: len(w_tokenizer.tokenize(x)))\n",
    "\n",
    "# dataset['sentences'] = dataset['reviewText'].apply(lambda x: [i.strip() for i in x.split(\".\")])\n",
    "\n",
    "# dataset['sentence_word_density'] = dataset['reviewText'].apply(lambda x: len(w_tokenizer.tokenize(x))/ len(x.split(\".\")))\n",
    "\n",
    "# dataset\n",
    "\n",
    "# dataset.to_csv(\"reviews_Musical_Instruments_5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = {}\n",
    "\n",
    "# for idx, i in enumerate(dataset[8].values):\n",
    "#     for j in i:\n",
    "#         try:\n",
    "#             df[j] += [idx]\n",
    "#         except:\n",
    "#             df[j] = [idx]\n",
    "\n",
    "# for i in df.keys():\n",
    "#     df[i] = len(list(set(df[i])))\n",
    "\n",
    "# df_vector = []\n",
    "# for i in dataset[8].values:\n",
    "#     d = [0]*len(vocabulary.keys())\n",
    "#     for j in i:\n",
    "#         if j in vocabulary.keys():\n",
    "#             d[vocabulary[j]] = df[j]\n",
    "#     df_vector.append(d)\n",
    "\n",
    "# np.array(df_vector).dump(\"df_vector_mrf_lda_first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_threshold = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# edge_dict, ignored, taken, count = {}, [], [], 0\n",
    "# for idxi, i in enumerate(vocabulary.keys()):\n",
    "#     print(idxi)\n",
    "#     for idxj, j in enumerate(vocabulary.keys()):\n",
    "#         if i != j:\n",
    "#             try:\n",
    "#                 a = embeddings_index[i]\n",
    "#                 b = embeddings_index[j]\n",
    "#                 if get_cosine(a, b) > edges_threshold:\n",
    "#                     try:\n",
    "#                         edge_dict[vocabulary[i]] += [vocabulary[j]]\n",
    "#                         edge_dict[vocabulary[j]] += [vocabulary[i]]\n",
    "#                     except:\n",
    "#                         edge_dict[vocabulary[i]] = [vocabulary[j]]\n",
    "#                         edge_dict[vocabulary[j]] = [vocabulary[i]]\n",
    "#             except:\n",
    "#                 try:\n",
    "#                     embeddings_index[i]\n",
    "#                     taken.append(i)\n",
    "#                 except:\n",
    "#                     ignored.append(i)\n",
    "#                 try:\n",
    "#                     embeddings_index[j]\n",
    "#                 except:\n",
    "#                     ignored.append(j)\n",
    "#                     taken.append(j)\n",
    "#                 pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs_edges, ignored, taken, count = [], [], [], 0\n",
    "for idx, doc in enumerate(dataset[8].values):\n",
    "    edges = []\n",
    "    print(idx)\n",
    "    for i in doc:\n",
    "        for j in doc:\n",
    "            if i != j:\n",
    "                try:\n",
    "                    a = embeddings_index[i]\n",
    "                    b = embeddings_index[j]\n",
    "                    if get_cosine(a, b) > edges_threshold and (vocabulary[i], vocabulary[j]) not in edges and (vocabulary[j], vocabulary[i]) not in edges:\n",
    "                        edges.append((vocabulary[i], vocabulary[j]))\n",
    "                except:\n",
    "                    try:\n",
    "                        embeddings_index[i]\n",
    "                        taken.append(i)\n",
    "                    except:\n",
    "                        ignored.append(i)\n",
    "                    try:\n",
    "                        embeddings_index[j]\n",
    "                    except:\n",
    "                        ignored.append(j)\n",
    "                        taken.append(j)\n",
    "                    pass\n",
    "    docs_edges.append(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"resources/docs_edges_electronics.pickle\",\"wb\")\n",
    "pickle.dump(docs_edges, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"resources/docs_edges_electronics.pickle\",\"rb\")\n",
    "docs_edges = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_dict = {}\n",
    "for i in docs_edges:\n",
    "    for j in i:\n",
    "        try:\n",
    "            edge_dict[j[0]] += [j[1]]\n",
    "            edge_dict[j[1]] += [j[0]]\n",
    "        except:\n",
    "            edge_dict[j[0]] = [j[1]]\n",
    "            edge_dict[j[1]] = [j[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in edge_dict.keys():\n",
    "    edge_dict[i] = list(set(edge_dict[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edge_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = N_TOPICS = 5\n",
    "N_SENTIMENT = 5\n",
    "lambda_param = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imp.reload(lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(dataset['overall'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = lda.LdaSampler(n_sentiment = N_SENTIMENT, n_topics=N_TOPICS, lambda_param=lambda_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(i)\n",
    "    sampler.run(count_matrix, ratings, edge_dict, maxiter=20)\n",
    "    print(sampler.loglikelihood(docs_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_words = sampler.getTopKWords(5, words)\n",
    "top_words = [t_words[i] for i in t_words.keys()]\n",
    "document_topic = sampler.theta().argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coherence_score(count_matrix, top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_hscore(sampler.theta(), count_matrix, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(sampler, 'sampler_first.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_matrix = (sampler.wordOccuranceMatrix > 0).astype(int)\n",
    "word_freq = idf_matrix.sum(axis=0)\n",
    "idf_matrix = idf_matrix * word_freq\n",
    "idf_matrix = idf_matrix * 1.0/idf_matrix.shape[0]\n",
    "document_topic_entropy = scipy.stats.entropy(sampler.dt_distribution.transpose())\n",
    "word_len = sampler.wordOccuranceMatrix.sum(axis=1)\n",
    "print (np.corrcoef(document_topic_entropy, word_len))\n",
    "#document_topic_entropy_len_normalized = document_topic_entropy * np.sqrt(word_len/2)\n",
    "document_popularity = np.true_divide(idf_matrix.sum(1),(idf_matrix != 0).sum(1)) #scipy.stats.mstats.gmean(idf_matrix,axis=1)\n",
    "print (np.corrcoef(document_topic_entropy, document_popularity))\n",
    "normalized_dts = sampler.dts_distribution * sampler.dt_distribution[:,:,np.newaxis]\n",
    "normalized_dts /= normalized_dts.sum(axis=-1)[:,:,np.newaxis]\n",
    "document_topic_sentiment_crossentropy = np.array([[scipy.stats.entropy(j) for j in i] for i in normalized_dts])\n",
    "document_ts_entropy_min = document_topic_sentiment_crossentropy.min(axis=1)\n",
    "document_ts_entropy_mean = document_topic_sentiment_crossentropy.mean(axis=1)\n",
    "document_ts_entropy_var = np.sqrt(document_topic_sentiment_crossentropy.var(axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
