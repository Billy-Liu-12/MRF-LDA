{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from num2words import num2words\n",
    "from collections import Counter\n",
    "from scipy import spatial\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from scipy.special import gammaln\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "\n",
    "import imp\n",
    "import lda2\n",
    "import scipy\n",
    "import operator\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processReviews(reviews, window=5, MAX_VOCAB_SIZE=1000):\n",
    "    vectorizer = CountVectorizer(analyzer=\"word\",tokenizer=None)\n",
    "    # preprocessor=None,stop_words=\"english\",max_features=MAX_VOCAB_SIZE,max_df=.5,min_df=5)\n",
    "    train_data_features = vectorizer.fit_transform(reviews)\n",
    "    words = vectorizer.get_feature_names()\n",
    "    vocabulary = dict(zip(words,np.arange(len(words))))\n",
    "    inv_vocabulary = dict(zip(np.arange(len(words)),words))\n",
    "    wordOccurenceMatrix = train_data_features.toarray()\n",
    "    return wordOccurenceMatrix, vocabulary, words\n",
    "\n",
    "def get_cosine(a, b):\n",
    "    return 1 - spatial.distance.cosine(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coherence_score(X, topic_sentiment_df):\n",
    "    X[X>1] = 1    \n",
    "    totalcnt = len(topic_sentiment_df)\n",
    "    total = 0\n",
    "    for allwords in topic_sentiment_df:\n",
    "        for word1 in allwords:\n",
    "            for word2 in allwords:\n",
    "                if word1 != word2:\n",
    "                    ind1 = vocabulary[word1]\n",
    "                    ind2 = vocabulary[word2]\n",
    "                    total += np.log((np.matmul(X[:,ind1].T, X[:,ind2]) + 1.0)/np.sum(X[:,ind2]))\n",
    "    return total/(2*totalcnt)\n",
    "\n",
    "def kl_score(pk,qk):\n",
    "    return (scipy.stats.entropy(pk,qk)*.5 + scipy.stats.entropy(qk,pk)*.5)\n",
    "\n",
    "def get_hscore(dt_distribution, X, k):\n",
    "    testlen = X.shape[0]\n",
    "    all_kl_scores = np.zeros((testlen, testlen))\n",
    "    for i in range(testlen-1):\n",
    "        for j in range(i+1,testlen):\n",
    "            score = kl_score(dt_distribution[i],dt_distribution[j])\n",
    "            all_kl_scores[i,j] = score\n",
    "            all_kl_scores[j,i] = score\n",
    "\n",
    "    dt = np.zeros((X.shape[0], k))\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        dt[i, dt_distribution[i].argmax()]=1\n",
    "\n",
    "    intradist = 0\n",
    "    for i in range(k):\n",
    "        cnt = dt[:,i].sum()\n",
    "        tmp = np.outer(dt[:,i],dt[:,i])\n",
    "        tmp = tmp * all_kl_scores\n",
    "        intradist += tmp.sum()*1.0/(cnt*(cnt-1))\n",
    "#         print(cnt, tmp.sum(), intradist)\n",
    "    intradist = intradist/k\n",
    "    \n",
    "\n",
    "    interdist = 0\n",
    "    for i in range(k):\n",
    "       for j in range(k):\n",
    "           if i != j:\n",
    "             cnt_i = dt[:,i].sum()\n",
    "             cnt_j = dt[:,j].sum()\n",
    "             tmp = np.outer(dt[:,i], dt[:,j])\n",
    "             tmp = tmp * all_kl_scores\n",
    "             interdist += tmp.sum()*1.0/(cnt_i*cnt_j)\n",
    "    interdist = interdist/(k*(k-1))\n",
    "    return intradist/interdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "embeddings_index = gensim.models.KeyedVectors.load_word2vec_format('pubmed2018_w2v_200D/pubmed2018_w2v_200D.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "icd_words = pickle.load(open(\"unique_words.pickle\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(\"dataset_cleaned_autocorrected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[8] = dataset[7].apply(lambda x: [item for item in x.split(\" \") if item in icd_words])\n",
    "dataset[9] = dataset[8].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_pickle(\"dataset_cleaned_autocorrected_medical_only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix, vocabulary, words = processReviews(dataset[9].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def grid_search(N_TOPICS, lambda_param, edges_threshold):\n",
    "    k = N_TOPICS\n",
    "    docs_edges = []\n",
    "    count = 0\n",
    "    ignored = []\n",
    "    taken = []\n",
    "    for idx, doc in enumerate(dataset[8].values):\n",
    "        edges = []\n",
    "        for i in doc:\n",
    "            for j in doc:\n",
    "                if i != j:\n",
    "                    try:\n",
    "                        a = embeddings_index[i]\n",
    "                        b = embeddings_index[j]\n",
    "                        if get_cosine(a, b) > edges_threshold:\n",
    "                            edges.append((vocabulary[i], vocabulary[j]))\n",
    "                    except:\n",
    "                        try:\n",
    "                            embeddings_index[i]\n",
    "                            taken.append(i)\n",
    "                        except:\n",
    "                            ignored.append(i)\n",
    "                        try:\n",
    "                            embeddings_index[j]\n",
    "                        except:\n",
    "                            ignored.append(j)\n",
    "                            taken.append(j)\n",
    "                        pass\n",
    "        docs_edges.append(edges)\n",
    "\n",
    "    edge_dict = {}\n",
    "    for i in docs_edges:\n",
    "        for j in i:\n",
    "            try:\n",
    "                edge_dict[j[0]] += [j[1]]\n",
    "            except:\n",
    "                edge_dict[j[0]] = [j[1]]\n",
    "    sampler = lda2.LdaSampler(n_topics=N_TOPICS, lambda_param=lambda_param)\n",
    "\n",
    "    for it, phi in enumerate(sampler.run(matrix, edge_dict)):\n",
    "        sampler.loglikelihood(docs_edges)\n",
    "\n",
    "\n",
    "\n",
    "    t_words = sampler.getTopKWords(5, words)\n",
    "    top_words = [t_words[i] for i in t_words.keys()]\n",
    "\n",
    "    return coherence_score(matrix, top_words), get_hscore(sampler.theta(), matrix, k), t_words, sampler.theta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOPICS = [3, 7, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_threshold = [0.3, 0.5, 0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_param = [0.3, 0.7, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump = []\n",
    "# for i in N_TOPICS:\n",
    "#     for j in edges_threshold:\n",
    "#         for k in lambda_param:\n",
    "#             dump.append((i, j, k, grid_search(N_TOPICS=i, edges_threshold=j, lambda_param=k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, i in enumerate(dump):\n",
    "#     if i[3][0] > -6 and i[3][1] < 0.3:\n",
    "#         print(idx, i[0], i[1], i[2], i[3][0], i[3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, i in enumerate(dump):\n",
    "#     if idx == 23:\n",
    "#         for j in i[3][2]:\n",
    "#             print(i[3][2][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "results =  grid_search(N_TOPICS=10, edges_threshold=0.7, lambda_param=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, u'area', [u'area', u'carcinoma', u'bilateral', u'neck', u'node'])\n",
      "(0, u'consciousness', [u'consciousness', u'event', u'free', u'chemotherapy', u'cycle'])\n",
      "(2, u'front', [u'house', u'sepsis', u'front', u'respiratory', u'baby'])\n",
      "(1, u'arch', [u'stage', u'arch', u'central', u'carcinoma', u'lower'])\n",
      "(0, u'absent', [u'absent', u'new', u'valve', u'presenting', u'drug'])\n",
      "(0, u'oriented', [u'oriented', u'category', u'lung', u'surgical', u'visit'])\n",
      "(1, u'inguinal', [u'ray', u'inguinal', u'multiple', u'closed', u'cell'])\n",
      "(4, u'lap', [u'skin', u'hiv', u'appendix', u'urine', u'lap'])\n",
      "(0, u'angioplasty', [u'angioplasty', u'mild', u'dominant', u'proximal', u'disease'])\n",
      "(0, u'due', [u'due', u'calculus', u'count', u'blood', u'renal'])\n"
     ]
    }
   ],
   "source": [
    "for i in results[2]:\n",
    "    a = (results[2][i])\n",
    "    avg_dist = []\n",
    "    for i in a:\n",
    "        sum = 0\n",
    "        for  j in a:\n",
    "            if i!=j:\n",
    "                distance = get_cosine(embeddings_index[i], embeddings_index[j])\n",
    "                sum += distance\n",
    "        avg_dist.append(sum)\n",
    "    print(np.array(avg_dist).argmin(), a[np.array(avg_dist).argmin()], a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [u'area', u'carcinoma', u'bilateral', u'neck', u'node'],\n",
       " 1: [u'consciousness', u'event', u'free', u'chemotherapy', u'cycle'],\n",
       " 2: [u'house', u'sepsis', u'front', u'respiratory', u'baby'],\n",
       " 3: [u'stage', u'arch', u'central', u'carcinoma', u'lower'],\n",
       " 4: [u'absent', u'new', u'valve', u'presenting', u'drug'],\n",
       " 5: [u'oriented', u'category', u'lung', u'surgical', u'visit'],\n",
       " 6: [u'ray', u'inguinal', u'multiple', u'closed', u'cell'],\n",
       " 7: [u'skin', u'hiv', u'appendix', u'urine', u'lap'],\n",
       " 8: [u'angioplasty', u'mild', u'dominant', u'proximal', u'disease'],\n",
       " 9: [u'due', u'calculus', u'count', u'blood', u'renal']}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
